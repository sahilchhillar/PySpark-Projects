{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Yelp Rating Analysis\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sahils-air:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Yelp Rating Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f93928b4250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars  sentiment\n",
       "0  Total bill for this horrible service? Over $8G...    1.0          0\n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...    5.0          1\n",
       "2  I have to say that this office really has it t...    5.0          1\n",
       "3  Went in for a lunch. Steak sandwich was delici...    5.0          1\n",
       "4  Today was my second out of three sessions I ha...    1.0          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df = pd.read_csv('yelp_ratings.csv')\n",
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- sentiment: long (nullable = true)\n",
      "\n",
      "+--------------------+-----+---------+\n",
      "|                text|stars|sentiment|\n",
      "+--------------------+-----+---------+\n",
      "|Total bill for th...|  1.0|        0|\n",
      "|I *adore* Travis ...|  5.0|        1|\n",
      "|I have to say tha...|  5.0|        1|\n",
      "|Went in for a lun...|  5.0|        1|\n",
      "|Today was my seco...|  1.0|        0|\n",
      "|I'll be the first...|  4.0|        1|\n",
      "|This place has go...|  1.0|        0|\n",
      "|I was really look...|  2.0|        0|\n",
      "|Like walking back...|  4.0|        1|\n",
      "|Walked in around ...|  1.0|        0|\n",
      "+--------------------+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_spark_df = spark.createDataFrame(yelp_df)\n",
    "yelp_spark_df.printSchema()\n",
    "yelp_spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In out text, we can have a lot of punctuations, hashtags, hyperlinks etc\n",
    "# These are irrelevant to our model and won't give us any information about the sentiment\n",
    "# So, we need to remove all those things form the text and get only relevant information\n",
    "# There are 2 ways to perform pre-processing\n",
    "# 1. By creating a custom function\n",
    "# 2. By using PySpark functions\n",
    "\n",
    "# We will use PySpark inbuilt function to perform the analysis and create our custom methods as well\n",
    "# Like this, we can reduce the number of errors and line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from langdetect) (1.15.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for whitespaces\n",
    "def check_blanks(data_str):\n",
    "    is_blank = str(data_str.isspace()) \n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# To determine whether the laguage of the text is english or not\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Removing hyperlinks, punctuations, number, mentions etc\n",
    "def remove_features(data_str): # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?') \n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters \n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word \n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word \n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sahilchhillar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the stop words\n",
    "stops = set(stopwords.words(\"english\")) \n",
    "\n",
    "def remove_stops(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stops:\n",
    "            # rebuild cleaned_str \n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word \n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sahilchhillar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Tagging the text\n",
    "def tag_and_remove(data_str):\n",
    "    cleaned_str = ' '\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS'] \n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] \n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "\n",
    "    # break string into 'words'\n",
    "    text = data_str.split()\n",
    "    # tag the text and keep only those with the right tags\n",
    "    tagged_text = pos_tag(text)\n",
    "    for tagged_word in tagged_text:\n",
    "        if tagged_word[1] in nltk_tags: \n",
    "            cleaned_str += tagged_word[0] + ' '\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sahilchhillar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "# Lemmatization of text\n",
    "def lemmatize(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer() \n",
    "    text = data_str.split() \n",
    "    tagged_words = pos_tag(text) \n",
    "    for word in tagged_words:\n",
    "        if 'v' in word[1].lower():\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
    "        else:\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
    "        if list_pos == 0: \n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "                cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1 \n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create UDF functions which stands for User Defined Functions\n",
    "# These are used to create reusable functions in Spark\n",
    "# Once created these functions can be later used in multiple DataFrames and SQL\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# We will register all our custom functions to these User Defined Functions for reusability\n",
    "check_lang_udf = udf(detect_language, StringType())\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(lemmatize, StringType())\n",
    "check_blanks_udf = udf(check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cols = yelp_spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+----+\n",
      "|                text|stars|sentiment|lang|\n",
      "+--------------------+-----+---------+----+\n",
      "|Total bill for th...|  1.0|        0|  en|\n",
      "|I *adore* Travis ...|  5.0|        1|  en|\n",
      "|I have to say tha...|  5.0|        1|  en|\n",
      "|Went in for a lun...|  5.0|        1|  en|\n",
      "|Today was my seco...|  1.0|        0|  en|\n",
      "+--------------------+-----+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Language Identification\n",
    "lang_df = yelp_spark_df.withColumn(\"lang\", check_lang_udf(yelp_spark_df[\"text\"]))\n",
    "lang_df = lang_df.filter(lang_df[\"lang\"] == \"en\")\n",
    "lang_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44310"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+--------------------+\n",
      "|                text|stars|sentiment|           stop_text|\n",
      "+--------------------+-----+---------+--------------------+\n",
      "|Total bill for th...|  1.0|        0|Total bill horrib...|\n",
      "|I *adore* Travis ...|  5.0|        1|I *adore* Travis ...|\n",
      "|I have to say tha...|  5.0|        1|I say office real...|\n",
      "|Went in for a lun...|  5.0|        1|Went lunch. Steak...|\n",
      "|Today was my seco...|  1.0|        0|Today second thre...|\n",
      "+--------------------+-----+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing all the stop words\n",
    "rm_stops_df = lang_df.select(raw_cols).withColumn(\"stop_text\", remove_stops_udf(lang_df[\"text\"]))\n",
    "rm_stops_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "|                text|stars|sentiment|           stop_text|           feat_text|\n",
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "|Total bill for th...|  1.0|        0|Total bill horrib...|total bill horrib...|\n",
      "|I *adore* Travis ...|  5.0|        1|I *adore* Travis ...|  adore travis ha...|\n",
      "|I have to say tha...|  5.0|        1|I say office real...|  say office real...|\n",
      "|Went in for a lun...|  5.0|        1|Went lunch. Steak...|went lunch steak ...|\n",
      "|Today was my seco...|  1.0|        0|Today second thre...|today second thre...|\n",
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing irrelevant features\n",
    "rm_features_df = rm_stops_df.select(raw_cols+[\"stop_text\"])\\\n",
    "                            .withColumn(\"feat_text\", \\\n",
    "                                remove_features_udf(rm_stops_df[\"stop_text\"]))\n",
    "rm_features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "|                text|stars|sentiment|           feat_text|         tagged_text|\n",
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "|Total bill for th...|  1.0|        0|total bill horrib...| total bill horri...|\n",
      "|I *adore* Travis ...|  5.0|        1|  adore travis ha...| travis hard rock...|\n",
      "|I have to say tha...|  5.0|        1|  say office real...| say office organ...|\n",
      "|Went in for a lun...|  5.0|        1|went lunch steak ...| went lunch steak...|\n",
      "|Today was my seco...|  1.0|        0|today second thre...| today second ses...|\n",
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tagging the words\n",
    "tagged_df = rm_features_df.select(raw_cols+[\"feat_text\"]) \\\n",
    "                          .withColumn(\"tagged_text\", \\\n",
    "                            tag_and_remove_udf(rm_features_df.feat_text))\n",
    "tagged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "|                text|stars|sentiment|         tagged_text|           lemm_text|\n",
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "|Total bill for th...|  1.0|        0| total bill horri...|total bill horrib...|\n",
      "|I *adore* Travis ...|  5.0|        1| travis hard rock...|travis hard rock ...|\n",
      "|I have to say tha...|  5.0|        1| say office organ...|say office organi...|\n",
      "|Went in for a lun...|  5.0|        1| went lunch steak...|go lunch steak sa...|\n",
      "|Today was my seco...|  1.0|        0| today second ses...|today second sess...|\n",
      "+--------------------+-----+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing words\n",
    "lemm_df = tagged_df.select(raw_cols+[\"tagged_text\"]) \\\n",
    "                   .withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"])) \n",
    "lemm_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lemm_df.select('lemm_text', 'sentiment')\n",
    "training, testing = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer \n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF and ML model\n",
    "tokenizer = Tokenizer(inputCol=\"lemm_text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer first\n",
    "vectorizer = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"cv_features\") \n",
    "# idf = IDF(minDocFreq=3, inputCol=\"words\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Machine Learning model\n",
    "# As this is a Binary Classification problem, we will use NaiveBayes model first\n",
    "nb = NaiveBayes(featuresCol=\"cv_features\", labelCol=\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pipeline\n",
    "nb_pipe = Pipeline(stages=[tokenizer, vectorizer, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = nb_pipe.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nb_model.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6222714256346977\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"sentiment\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"cv_features\", labelCol=\"sentiment\")\n",
    "lr_pipe = Pipeline(stages=[tokenizer, vectorizer, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr_pipe.fit(training)\n",
    "lr_pred = lr_model.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9402538281394938\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"sentiment\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(lr_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
